---
title: "Hands-on Exercise 8"
subtitle: "Geographical Segmentation with Spatially Constrained Clustering Techniques (Continuation of Hands-on Exercise 7)"
author: "Kai Feng"
date: "Oct 19, 2024"
date-modified: "last-modified"
execute:
  eval: true
  echo: true
  freeze: true
---

# **1.0 Overview**

This exercise focuses on using geographically referenced multivariate data to delineate homogeneous regions. Two major analyses are used:

1.  Hierarchical cluster analysis

2.  Spatially constrained cluster analysis

The goal is to segment Shan State, Myanmar, into homogeneous regions based on multiple Information and Communication Technology (ICT) indicators: Radio, Television, Landline phone, Mobile phone, Computer, and Internet at home. This approach is commonly applied in geobusiness and spatial policy for defining market or planning areas.

<br/><br/>

# **2.0 Setup**

## 2.1 Installing R-Packages

Spatial data handling

-   **sf**, **rgdal** and **spdep**

Attribute data handling

-   **tidyverse**, especially **readr**, **ggplot2** and **dplyr**

Choropleth mapping

-   **tmap**

Multivariate data visualisation and analysis

-   **coorplot**, **ggpubr**, and **heatmaply**

Cluster analysis

-   **cluster** and **ClustGeo**

```{r}
pacman::p_load(spdep, tmap, sf, ClustGeo, 
               ggpubr, cluster, factoextra, NbClust,
               heatmaply, corrplot, psych, tidyverse, GGally)
```

<br/>

## 2.2 Data Acquisition

Two data sets will be used:

-   `Myanmar Township Boundary Data`: a GIS data set in ESRI shapefile format. It consists of township boundary information of Myanmar. The spatial data are captured in polygon features.

-   `Shan-ICT.csv`: an extract of [**The 2014 Myanmar Population and Housing Census Myanmar**](https://myanmar.unfpa.org/en/publications/2014-population-and-housing-census-myanmar-data-sheet) at the township level.

::: panel-tabset
# **Importing Geospatial Data**

```{r}
shan_sf <- st_read(dsn = "data/geospatial", 
                   layer = "myanmar_township_boundaries") %>%
  filter(ST %in% c("Shan (East)", "Shan (North)", "Shan (South)")) %>%
  select(c(2:7))

shan_sf

glimpse(shan_sf)
```

# Importing Aspatial Data

```{r}
ict <- read_csv ("data/aspatial/Shan-ICT.csv")

summary(ict)
```
:::

<br/><br/>

# **3.0 Spatially Constrained Clustering: SKATER approach**

```{r}
# Convert into SpatialPolygonsDataFrame
shan_sp <- as_Spatial(shan_sf)
```

::: callout-note
Convert because *`SKATER`* only supports sp obects like SpatialPolygonsDataFrame
:::

::: panel-tabset
## Computing Neighbour List

```{r}
shan.nb <- poly2nb(shan_sp)
summary(shan.nb)
```

**Plot neighbours list**

```{r}
coords <- st_coordinates(
  st_centroid(st_geometry(shan_sf)))

plot(st_geometry(shan_sf), # boundaries
     border=grey(.5))
plot(shan.nb, # neighbour list object, with coordinates applied to the original SpatialPolygonDataFrame (Shan state township boundaries) to extract the centroids of the polygons. These are used as the nodes for the graph representation. 
     coords, 
     col="blue", 
     add=TRUE) # in order to plot the network on top of the boundaries
```

::: callout-note
If plot network first, some areas will be clipped.

Because the plotting area is determined by the characteristics of the first plot.

In this example, the boundary map extends further than the graph, so plot boundary first.
:::

## Computing minimum spanning tree

To calculate edge cost (distance between nodes):

```{r}
shan_ict <- read_rds("data/rds/shan_ict.rds")

lcosts <- nbcosts(shan.nb, shan_ict)
head(lcosts)
```

::: callout-note
Each observation gives pairwise dissimilarity between its values on the 5 variables & the values for the neighbouring observation (from the neighbour list.

Basically this is the notion of a generalised weight for a spatial weights matrix
:::

<br/>

-   Incorporate these costs into a weighted neighbor list.

-   Convert the neighbor list to a weights object using the computed `lcosts` as weights.

```{r}
shan.w <- nb2listw(shan.nb, 
                   lcosts, 
                   style="B") # to make sure the cost values are not row-standardised
summary(shan.w)
```

## Computing minimum spanning tree

-   Computed by mean of mstree()

```{r}
shan.mst <- mstree(shan.w)
class(shan.mst)
dim(shan.mst)
head(shan.mst)
```

::: callout-note
The dimension is 54 not 55. Because the min spanning tree consists on n-1 edges (links) in order to traverse all the nodes
:::

**Plot MST**

```{r}
plot(st_geometry(shan_sf), 
                 border=gray(.5))
plot.mst(shan.mst, 
         coords, 
         col="blue", 
         cex.lab=0.7, 
         cex.circles=0.005, 
         add=TRUE)
```

::: callout-note
We can see how the initial neighbour list is simplified to just 1 edge connecting each of the nodes while passing through all the nodes
:::

## Computing spatially constrained clusters using SKATER method

```{r}
clust6 <- spdep::skater(edges = shan.mst[,1:2], 
                 data = shan_ict, 
                 method = "euclidean", 
                 ncuts = 5)
str(clust6)
```

::: callout-note
The *skater()* takes three mandatory arguments: - the first two columns of the MST matrix (i.e. not the cost), - the data matrix (to update the costs as units are being grouped), and - the number of cuts. Note: It is set to **one less than the number of clusters**. So, the value specified is **not** the number of clusters, but the number of cuts in the graph, one less than the number of clusters.

The most interesting component of this list structure is the groups vector containing the labels of the cluster to which each observation belongs (as before, the label itself is arbitary). This is followed by a detailed summary for each of the clusters in the edges.groups list. Sum of squares measures are given as ssto for the total and ssw to show the effect of each of the cuts on the overall criterion.
:::

**Check cluster assignment:**

```{r}
ccs6 <- clust6$groups
ccs6

table(ccs6)
```

::: callout-note
We can find out how many observations are in each cluster by means of the table command. Parenthetially, we can also find this as the dimension of each vector in the lists contained in edges.groups. For example, the first list has node with dimension 12, which is also the number of observations in the first cluster.
:::

**Plot the pruned tree that shows 5 clusters on top of townshop area**

```{r}
plot(st_geometry(shan_sf), 
     border=gray(.5))
plot(clust6, 
     coords, 
     cex.lab=.7,
     groups.colors=c("red","green","blue", "brown", "pink"),
     cex.circles=0.005, 
     add=TRUE)
```

**Visualising the clusters in choropleth map**

```{r}
shan_sf_cluster  <- read_rds("data/rds/shan_sf_cluster.rds")

groups_mat <- as.matrix(clust6$groups)
shan_sf_spatialcluster <- cbind(shan_sf_cluster, as.factor(groups_mat)) %>%
  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)
qtm(shan_sf_spatialcluster, "SP_CLUSTER")
```

**Hierarchial clustering VS Spatially constrained hierarchical clustering**

```{r}
hclust.map <- qtm(shan_sf_cluster,
                  "CLUSTER") + 
  tm_borders(alpha = 0.5) 

shclust.map <- qtm(shan_sf_spatialcluster,
                   "SP_CLUSTER") + 
  tm_borders(alpha = 0.5) 

tmap_arrange(hclust.map, shclust.map,
             asp=NA, ncol=2)
```
:::

<br/><br/>

# **4.0 Spatially Constrained Clustering: ClustGeo Method**

::: callout-note
Using **ClustGeo** package.

designed to support the need of performing spatially constrained cluster analysis. More specifically, it provides a Ward-like hierarchical clustering algorithm called `hclustgeo()` including spatial/geographical constraints.

In the nutshell, the algorithm uses two dissimilarity matrices D0 and D1 along with a mixing parameter alpha, whereby the value of alpha must be a real number between \[0, 1\]. D0 can be non-Euclidean and the weights of the observations can be non-uniform. It gives the dissimilarities in the **attribute/clustering variable space**. D1, on the other hand, gives the dissimilarities in the **constraint space**. The criterion minimised at each stage is a convex combination of the homogeneity criterion calculated with D0 and the homogeneity criterion calculated with D1.

The idea is then to determine a value of alpha which increases the spatial contiguity without deteriorating too much the quality of the solution based on the variables of interest. This need is supported by a function called `choicealpha()`.
:::

### Ward-like hierarchical clustering: ClustGeo

similar to hclust()

```{r}
proxmat <- read_rds("data/rds/proxmat.rds")

nongeo_cluster <- hclustgeo(proxmat)
plot(nongeo_cluster, cex = 0.5)
rect.hclust(nongeo_cluster, 
            k = 6, 
            border = 2:5)
```

::: callout-note
Note that the dissimilarity matrix must be an object of class `dist`, i.e. an object obtained with the function `dist()`
:::

**Mapping the clusters formed**

```{r}
groups <- as.factor(cutree(nongeo_cluster, k=6))
shan_sf_ngeo_cluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER` = `as.matrix.groups.`)
qtm(shan_sf_ngeo_cluster, "CLUSTER")
```

**Spatially Constrained Hierarchical Clustering**

Before we can performed spatially constrained hierarchical clustering, a spatial distance matrix will be derived by using [`st_distance()`](https://r-spatial.github.io/sf/reference/geos_measures.html) of sf package.

```{r}
dist <- st_distance(shan_sf, shan_sf)
distmat <- as.dist(dist)
```

Next, `choicealpha()` will be used to determine a suitable value for the mixing parameter alpha as shown in the code chunk below.

```{r}
cr <- choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K=6, graph = TRUE)
```

```{r}
# With reference to the graphs above, alpha = 0.2 will be used:
clustG <- hclustgeo(proxmat, distmat, alpha = 0.2)

# derive the cluster object
groups <- as.factor(cutree(clustG, k=6))

# join back the group list with shan_sf polygon feature data frame
shan_sf_Gcluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER` = `as.matrix.groups.`)

qtm(shan_sf_Gcluster, "CLUSTER")
```

<br/><br/>

# **5.0 Visual Interpretation of Clusters**

### Visualising individual clustering variable

```{r}
#| eval: false

# reveal the distribution of a clustering variable (i.e RADIO_PR) by cluster
ggplot(data = shan_sf_ngeo_cluster,
       aes(x = CLUSTER, y = RADIO_PR)) +
  geom_boxplot()
```

![](https://r4gdsa.netlify.app/chap12_files/figure-html/unnamed-chunk-71-1.png)

::: callout-note
The boxplot reveals Cluster 3 displays the highest mean Radio Ownership Per Thousand Household. This is followed by Cluster 2, 1, 4, 6 and 5.
:::

### Multivariate Visualisation

Past studies shown that parallel coordinate plot can be used to reveal clustering variables by cluster very effectively. In the code chunk below, [`ggparcoord()`](https://ggobi.github.io/ggally/reference/ggparcoord.html) of [**GGally**](https://ggobi.github.io/ggally/index.html) package

```{r}
#| eval: false

ggparcoord(data = shan_sf_ngeo_cluster, 
           columns = c(17:21), 
           scale = "globalminmax",
           alphaLines = 0.2,
           boxplot = TRUE, 
           title = "Multiple Parallel Coordinates Plots of ICT Variables by Cluster") +
  facet_grid(~ CLUSTER) + 
  theme(axis.text.x = element_text(angle = 30))
```

![](https://r4gdsa.netlify.app/chap12_files/figure-html/unnamed-chunk-72-1.png)

The parallel coordinate plot above reveals that households in Cluster 4 townships tend to own the highest number of TV and mobile-phone. On the other hand, households in Cluster 5 tends to own the lowest of all the five ICT.

Note that the `scale` argument of `ggparcoor()` provide several methods to scale the clustering variables. They are:

-   std: univariately, subtract mean and divide by standard deviation.

-   robust: univariately, subtract median and divide by median absolute deviation.

-   uniminmax: univariately, scale so the minimum of the variable is zero, and the maximum is one.

-   globalminmax: no scaling is done; the range of the graphs is defined by the global minimum and the global maximum.

-   center: use uniminmax to standardize vertical height, then center each variable at a value specified by the scaleSummary param.

-   centerObs: use uniminmax to standardize vertical height, then center each variable at the value of the observation specified by the centerObsID param

There is no one best scaling method to use. You should explore them and select the one that best meet your analysis need.

Last but not least, we can also compute the summary statistics such as mean, median, sd, etc to complement the visual interpretation.

In the code chunk below, `group_by()` and `summarise()` of dplyr are used to derive mean values of the clustering variables.

```{r}
#| eval: false

shan_sf_ngeo_cluster %>% 
  st_set_geometry(NULL) %>%
  group_by(CLUSTER) %>%
  summarise(mean_RADIO_PR = mean(RADIO_PR),
            mean_TV_PR = mean(TV_PR),
            mean_LLPHONE_PR = mean(LLPHONE_PR),
            mean_MPHONE_PR = mean(MPHONE_PR),
            mean_COMPUTER_PR = mean(COMPUTER_PR))
```
